{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Email Detection\n",
    "## Author: Hexing Ren\n",
    "### Click [here](http://www.hexingren.com/practical-data-science) to go back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this project, we will classify emails as either spam or not spam using support vector machines. The full dataset consists 80k labeled emails. The labels are 1 if they are ham (not spam), and -1 if they are spam. The lines of the emails have already been slightly processed, such that different words are space delimited, however little other processing has occurred. \n",
    "\n",
    "## Preliminary notes\n",
    "1. Scikit-learn is allowed to use. \n",
    "2. For this project, each proceeding part depends on the previous since we are building up a moderately sized data science pipeline. Verify your previous parts before proceeding onto the next. \n",
    "3. Similar the linear regression notebook of the previous project, you will need to use the tfidf function from the natural language processing notebook.\n",
    "4. As we move into more advanced algorithms and techniques, there will be more introductions of randomness. This means that some of the example outputs in the notebook contain some randomness, and will probably not match the results exactly. Verify the code by checking the properties/invariants or feeding in static inputs for which we can calculate the output. \n",
    "5. When writing pickle files to be read into Autolab, **write files with the binary flag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "import scipy.optimize\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "with open(\"X1.txt\") as f:\n",
    "    emails = f.readlines()\n",
    "labels = np.loadtxt(\"y1.txt\")\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "from natural_language_processing import tfidf\n",
    "features, all_words = tfidf(emails)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification\n",
    "Recall the support vector machine (SVM) from slide 17 of linear classification. Since it is such a straightforward algorithm, we will implement it below.\n",
    "\n",
    "### Specifications\n",
    "1. If you do not use matrix operations, your code will be **very slow**. Every function in here can be implemented in 1 or 2 lines using matrix equations, and the only for loop you need is the training loop for gradient descent. **If your code is slow here, it will be extremely slow in the next section when doing parameter search**.\n",
    "2. You should train your SVM using gradient descent as described in the slides. Your objective value should also mimic that of the slides. \n",
    "3. Since this is a convex function, your gradient steps should always decrease your objective. A simple check when writing these optimization procedures is to print your objectives and verify that this is the case (or plot them with matplotlib).\n",
    "4. You can also use scipy.optimize.check_grad to numerically verify the correctness of your gradients. \n",
    "5. For the unlikely boundary case where your hypothesis outputs 0, we will treat that as a positive prediction. \n",
    "6. Be careful of numpy.matrix objects which are constrained to always have dimension 2 (scipy operations will sometimes return this instead of an ndarray). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, X, y, reg):\n",
    "        \"\"\" Initialize the SVM attributes and initialize the weights vector to the zero vector. \n",
    "            Attributes: \n",
    "                X (array_like) : training data intputs\n",
    "                y (vector) : 1D numpy array of training data outputs\n",
    "                reg (float) : regularizer parameter\n",
    "                theta : 1D numpy array of weights\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.reg = reg\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        self.Xy = sp.diags(y).dot(X)\n",
    "    \n",
    "    def objective(self, X, y):\n",
    "        \"\"\" Calculate the objective value of the SVM. When given the training data (self.X, self.y), this is the \n",
    "            actual objective being optimized. \n",
    "            Args:\n",
    "                X (array_like) : array of examples, where each row is an example\n",
    "                y (array_like) : array of outputs for the training examples\n",
    "            Output:\n",
    "                (float) : objective value of the SVM when calculated on X,y\n",
    "        \"\"\"\n",
    "        Xy = sp.diags(y).dot(X)\n",
    "        dist = (-1) * Xy.dot(self.theta) + 1\n",
    "        obj = dist[dist > 0].sum() + (self.reg / 2) * (np.linalg.norm(self.theta)**2)\n",
    "        return obj\n",
    "\n",
    "    \n",
    "    def gradient(self):\n",
    "        \"\"\" Calculate the gradient of the objective value on the training examples. \n",
    "            Output:\n",
    "                (vector) : 1D numpy array containing the gradient\n",
    "        \"\"\"\n",
    "        grad = (-1) * ((self.Xy).T.dot((self.Xy).dot(self.theta) <= 1)) + self.reg * self.theta\n",
    "        return grad\n",
    "    \n",
    "    def train(self, niters=100, learning_rate=1, verbose=False):\n",
    "        \"\"\" Train the support vector machine with the given parameters. \n",
    "            Args: \n",
    "                niters (int) : the number of iterations of gradient descent to run\n",
    "                learning_rate (float) : the learning rate (or step size) to use when training\n",
    "                verbose (bool) : an optional parameter that you can use to print useful information (like objective value)\n",
    "        \"\"\"\n",
    "        for iter in range(niters):\n",
    "            self.theta = self.theta -  learning_rate * self.gradient()\n",
    "            if verbose:\n",
    "                print self.objective(self.X, self.y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict the class of each label in X. \n",
    "            Args: \n",
    "                X (array_like) : array of examples, where each row is an example\n",
    "            Output:\n",
    "                (vector) : 1D numpy array containing predicted labels\n",
    "        \"\"\"\n",
    "        y_pre = np.array(X.dot(self.theta))\n",
    "        y_pre[y_pre < 0] = -1\n",
    "        y_pre[y_pre >= 0] = +1\n",
    "        return y_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful tricks for debugging: \n",
    "1. Use very simple inputs (i.e. small vectors of ones) and compare the output of each function with a hand calculation. \n",
    "2. One way to guarantee your gradient is correct is to verify it numerically using a derivative approximation. You can read more about numerical differentiation methods here (https://en.wikipedia.org/wiki/Finite_difference) but for your purposes, you can use scipy.optimize.check_grad to do the numerical checking for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "# Verify the correctness of your code on small examples\n",
    "y0 = np.random.randint(0,2,5)*2-1\n",
    "X0 = np.random.random((5,10))\n",
    "t0 = np.random.random(10)\n",
    "svm0 = SVM(X0,y0, 1e-4)\n",
    "svm0.theta = t0\n",
    "\n",
    "\n",
    "# def obj(theta):\n",
    "#     pass\n",
    "\n",
    "# def grad(theta):\n",
    "#     pass\n",
    "\n",
    "# scipy.optimize.check_grad(obj, grad, t0)\n",
    "\n",
    "svm0.train(niters=100, learning_rate=1, verbose=True)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above small example, our solution gets a gradient error on the order of 1e-08 from scipy.optimize.check_grad. Your objective values should be monotonically decreasing. \n",
    "\n",
    "Once that works, try training your SVM on the tfidf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "svm = SVM(features, labels, reg = 1e-4)\n",
    "svm.train(niters=100, learning_rate=1, verbose=False)\n",
    "print (svm.predict(features) == labels).sum()\n",
    "print len(labels)\n",
    "print svm.predict(features)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation gets the following results:\n",
    "* For 100 iterations, regularization 1e-4, and learning rate 1.0, our solution is able to achieve perfect training classification accuracy (100% accuracy on the training data)\n",
    "* Training for 100 iterations takes about 2.13 seconds (measured using %timeit). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Cross validation and Parameter Grid Search\n",
    "As you may have noticed, there are parameters in the SVM learning algorithm that we chose somewhat arbitrarily: the regularization parameter and the learning rate (also technically the number of iterations for the learning algorithm, but you'll only consider the first two for simplicity). \n",
    "\n",
    "We were also able to achieve perfect training accuracy with these random parameters. This should make you suspicious: we have an enormous amount of features so it would be extremely easy to overfit to the data, so our model may not generalize well. \n",
    "\n",
    "You will now evaluate and select parameters using cross validation and grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ModelSelector:\n",
    "    \"\"\" A class that performs model selection. \n",
    "        Attributes:\n",
    "            blocks (list) : list of lists of indices of each block used for k-fold cross validation, e.g. blocks[i] \n",
    "            gives the indices of the examples in the ith block \n",
    "            test_block (list) : list of indices of the test block that used only for reporting results\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, P, k, niters):\n",
    "        \"\"\" Initialize the model selection with data and split into train/valid/test sets. Split the permutation into blocks \n",
    "            and save the block indices as an attribute to the model. \n",
    "            Args:\n",
    "                X (array_like) : array of features for the datapoints\n",
    "                y (vector) : 1D numpy array containing the output labels for the datapoints\n",
    "                P (vector) : 1D numpy array containing a random permutation of the datapoints\n",
    "                k (int) : number of folds\n",
    "                niters (int) : number of iterations to train for\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.k = k\n",
    "        self.niters = niters\n",
    "        \n",
    "        self.test_block = P[k * (len(P) / (k + 1)) : (k + 1) * (len(P) / (k + 1))]\n",
    "        \n",
    "        blocks = list()\n",
    "        for i in range(k):\n",
    "            block = P[i * (len(P) / (k + 1)) : (i + 1) * (len(P) / (k + 1))]\n",
    "            blocks.append(block)\n",
    "        self.blocks = blocks\n",
    "\n",
    "    def cross_validation(self, lr, reg):\n",
    "        \"\"\" Given the permutation P in the class, evaluate the SVM using k-fold cross validation for the given parameters \n",
    "            over the permutation\n",
    "            Args: \n",
    "                lr (float) : learning rate\n",
    "                reg (float) : regularizer parameter\n",
    "            Output: \n",
    "                (float) : the cross validated error rate\n",
    "        \"\"\"\n",
    "        avg_perf = 0.0\n",
    "\n",
    "        X_cv = [self.X[self.blocks[i]] for i in range(self.k)]\n",
    "        y_cv = [self.y[self.blocks[i]] for i in range(self.k)]\n",
    "        \n",
    "        for j in range(self.k):\n",
    "\n",
    "            X_train = sp.vstack(X_cv[ : j] + X_cv[j+1 : ])\n",
    "            y_train = np.hstack(y_cv[ : j] + y_cv[j+1 : ])\n",
    "\n",
    "            X_validate = X_cv[j]\n",
    "            y_validate = y_cv[j]\n",
    "\n",
    "            mod = SVM(X_train, y_train, reg)\n",
    "            mod.train(self.niters, lr)\n",
    "\n",
    "            y_pred = mod.predict(X_validate)\n",
    "            \n",
    "            accy = float(np.sum(y_pred != y_validate)) / float(X_validate.shape[0])\n",
    "            avg_perf = avg_perf + accy\n",
    "        \n",
    "        avg_perf = float(avg_perf) / float(self.k)\n",
    "        return avg_perf\n",
    "    \n",
    "    def grid_search(self, lrs, regs):\n",
    "        \"\"\" Given two lists of parameters for learning rate and regularization parameter, perform a grid search using\n",
    "            k-wise cross validation to select the best parameters. \n",
    "            Args:  \n",
    "                lrs (list) : list of potential learning rates\n",
    "                regs (list) : list of potential regularizers\n",
    "            Output: \n",
    "                (lr, reg) : 2-tuple of the best found parameters\n",
    "        \"\"\"\n",
    "        opt_params = (lrs[0], regs[0])\n",
    "        for i in range(len(lrs)):\n",
    "            for j in range(len(regs)):\n",
    "                (opt_lr, opt_reg) = opt_params\n",
    "                if self.cross_validation(lrs[i], regs[j]) < self.cross_validation(opt_lr, opt_reg):\n",
    "                    opt_params = (lrs[i], regs[j])\n",
    "        return opt_params\n",
    "    \n",
    "    def test(self, lr, reg):\n",
    "        \"\"\" Given parameters, calculate the error rate of the test data given the rest of the data. \n",
    "            Args: \n",
    "                lr (float) : learning rate\n",
    "                reg (float) : regularizer parameter\n",
    "            Output: \n",
    "                (err, svm) : tuple of the error rate of the SVM on the test data and the learned model\n",
    "        \"\"\"\n",
    "        X_training0 = [self.X[self.blocks[i]] for i in range(self.k)]\n",
    "        y_training0 = [self.y[self.blocks[i]] for i in range(self.k)]\n",
    "        X_test = self.X[self.test_block]\n",
    "        y_test = self.y[self.test_block]\n",
    "        \n",
    "        X_training = sp.vstack(X_training0[ : 3] + X_training0[3 : ])\n",
    "        y_training = np.hstack(y_training0[ : 3] + y_training0[3 : ])\n",
    "        \n",
    "        svm = SVM(X_training, y_training, reg)\n",
    "        svm.train(self.niters, lr)\n",
    "        y_pred = svm.predict(X_test)\n",
    "        err = float(np.sum(y_pred != y_test)) / float(y_pred.shape[0])\n",
    "        \n",
    "        return (err, svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation\n",
    "How can we evaluate our choice of parameters? One way is to perform k-fold cross validation, which operates as follows \n",
    "\n",
    "1. We split the data into k+1 randomly selected but uniformly sized pieces, and set aside one block for testing\n",
    "2. For each of the remaining k parts, we train the model on k-1 parts and validate our model on the heldout part. \n",
    "3. This gives k results, and the average of these runs gives the final result\n",
    "\n",
    "The idea is that by holding out part of the dataset as validation data, we can train and measure our generalization ability. Note the key fact here: the training does not see the validation data at all, which is why it measures generalization! Randomizing the groups removes bias from ordering (i.e. if these results occurred in chronological order, we don't want to train on only Monday's results to predict on Wednesday's results), and averaging over the groups reduces the variance. \n",
    "\n",
    "In this problem, we will use classification error rate as our result metric (so the fraction of times in which our model returns the wrong answer). Calculating this value via k-fold cross validation gives us a measure of how well our model generalizes to new data (lower error rate is better). \n",
    "\n",
    "### Specification\n",
    "1. Break the examples in k+1 groups as follows: \n",
    "    * break the permutation into blocks of size $\\text{ceil}\\left(\\frac{n}{k+1}\\right)$ (the last block may be shorter than the rest)\n",
    "    * set aside the k+1th group as the testing block, and use the remaining k blocks for cross validation\n",
    "    * use the permutation as indices to select the rows that correspond to that block\n",
    "    * Example: k=2, P=[1,3,2,4,5,6] sets aside [5,6] as the test set, and break the remaining permutation into [[1,3],[2,4]] so the blocks of data for validation are X[[1,3],:] and X[[2,4],:]\n",
    "    * the order of the indices in the blocks should match the order in the original permutation\n",
    "2. For each group k, train the model on all other datapoints, and compute the error rate on the held-out group. \n",
    "3. Return the average error rate over all k folds, along \n",
    "\n",
    "You can try it on the random dataset just to make sure it works, but you won't get anything meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "MS0 = ModelSelector(X0, y0, np.arange(X0.shape[0]), 3, 100)\n",
    "MS0.cross_validation(0.1, 1e-4)\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running this on the tfidf features. Can you achieve the same performance on the validation dataset as you did on the training data set? Remember to use a random permutation (you'll get noticeably different results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "# MS0 = ...\n",
    "# MS0.cross_validation(...)\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation returns results with mean classification error 0.01169 and standard deviation 0.0092 (over 10 different permutations). The parameters we used are k=5 folds for learning rate 1 and regularization 1e-4, when run for 100 iterations. Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "Now, we have a means of evaluating our choice of parameters. We can now combine this with a grid search over parameters to determine the best combination. Given two lists of parameters, we compute the classification error using k-fold cross validation for each pair of parameters, and output the parameters that produces the best validation result. \n",
    "\n",
    "### Specification\n",
    "1. Select the pair of hyperparamers that produces the smallest k-fold validation error. \n",
    "2. Train a new model using all the training and validation data\n",
    "3. Report the classification accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MS = ModelSelector(...)\n",
    "# lr, reg = MS.grid_search(...)\n",
    "# print lr, reg\n",
    "# print MS.test(lr,reg)\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "MS = ModelSelector(features, labels, np.arange(features.shape[0]), 4, 100)\n",
    "lr, reg = MS.grid_search(np.logspace(-1,1,3), np.logspace(-2,1,4))\n",
    "print lr, reg\n",
    "print MS.test(lr,reg)\n",
    "# AUTOLAB IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can try it on the randomized small example just to make sure your code runs, however it won't produce any sort of meaningful result. On our implementation, performing a grid search on learning rates [0.1, 1, 10] and regularization [0.01, 0.1, 1, 10] with 100 iterations for training results in a final test error rate of 0.0232 and selects a learning rate of 1, and a regularization parameter of 0.1. Our implementation takes about 1 minute and 7 seconds to perform the grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Compression\n",
    "While you are able to get decent results using an SVM and basic tf-idf features, there are 2 main problems here:\n",
    "1. The actual dataset is 8x larger than the one that you load at the start\n",
    "2. The number of features is extremely bloated and consumes a lot of space and computing power for a binary classification problem\n",
    "\n",
    "So the above methodology would actually take a lot of time and memory to run on the full dataset. Following the example you did in the text classification notebook, we would need to save the tf-idf matrix for the entire training dataset (which is enormous), and then use that to generate features on new examples. \n",
    "\n",
    "One way to tackle this is to generate fewer, but effective, features. For example, instead of generating full tf_idf features for every single word in every email, we can instead try to focus on keywords that frequently only occur in spam email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_frequent_indicator_words(docs, y, threshold):\n",
    "    spam_counts = Counter(w for (i, d) in enumerate(docs) for w in d.split() if y[i] == -1)\n",
    "    ham_counts = Counter(w for (i, d) in enumerate(docs) for w in d.split() if y[i] == 1)\n",
    "    spam_words = set(w for (i, d) in enumerate(docs) for w in d.split() if y[i] == -1)\n",
    "    ham_words = set(w for (i, d) in enumerate(docs) for w in d.split() if y[i] == 1)\n",
    "    \n",
    "    spam_dict = [w for w in (spam_words - ham_words) if spam_counts[w] >= threshold]\n",
    "    ham_dict = [w for w in (ham_words - spam_words) if ham_counts[w] >= threshold]\n",
    "    return spam_dict, ham_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "s,h = find_frequent_indicator_words(emails, labels, 50)\n",
    "with open('student_data.pkl', 'wb') as f:\n",
    "    pickle.dump((s,h), f)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation gets 2422 spam words and 290 ham words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Spam Detection\n",
    "\n",
    "Your goal here is to get at least 80% accuracy on spam detection in an efficient manner. If you are unsure of what to do, one way is to use the frequent indicator words implemented above and generate 2 features per emails: the number of spam indicator words and the number of ham indicator words for a total of two features. This is a huge dimensionality reduction!\n",
    "\n",
    "Of course, you don't have to do this. As long as you achieve at least 80% accuracy with your features you will receive the base credit for this problem. You are allowed to submit supplemental files. Make sure these supplemental files make it into your tar file (update the Makefile if you use it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def email2features(emails):\n",
    "    \"\"\" Given a list of emails, create a matrix containing features for each email. \"\"\"\n",
    "    with open('student_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    features = np.zeros((len(emails), 2))\n",
    "    for i, e in enumerate(emails):\n",
    "        count = Counter(w for w in e.split())\n",
    "        features[i, 0] = sum(count[s0] for s0 in data[0])\n",
    "        features[i, 1] = sum(count[s0] for s0 in data[1])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "small_features = email2features(emails)\n",
    "# MS = ModelSelector(...)\n",
    "# lr, reg = MS.grid_search(...)\n",
    "# print lr, reg\n",
    "# err, svm = MS.test(lr,reg)\n",
    "# print err\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Author: Hexing Ren\n",
    "### Click [here](http://www.hexingren.com/practical-data-science) to go back."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
